%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\usepackage{multibib}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
% for eps graphics

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Wenneker Article
% % LaTeX Template
% % Version 2.0 (28/2/17)
% %
% % This template was downloaded from:
% % http://www.LaTeXTemplates.com
% %
% % Authors:
% % Vel (vel@LaTeXTemplates.com)
% % Frits Wenneker
% %
% % License:
% % CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% %
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %----------------------------------------------------------------------------------------
% %	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
% %----------------------------------------------------------------------------------------

% \documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

% \input{structure.tex} % Specifies the document structure and loads requires packages

\usepackage{textalpha} % For unicode greek letters in normal text
\usepackage{amssymb, amsmath, amsthm, mathrsfs, enumitem, amsfonts, latexsym, bbm, stmaryrd, thmtools}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\definecolor{svlinks}{rgb}{.0,0.3,0.6} %tmavě modrá
\definecolor{ansa}{rgb}{1,0.3,0} %oranžová
%\usepackage[bookmarks,colorlinks=true,pdfhighlight=/O,linkcolor=svlinks,urlcolor=svlinks,citecolor=svlinks,
%         pdftitle={PyVallex: A Processing System for valency Lexicon Data},
%         pdfauthor={Anna Vernerová, Jonathan L. Verner},
%         pdfsubject={},
%         pdfkeywords={}
%         ]{hyperref}
%\usepackage{enumitem}
\usepackage{minted}

\newcommand{\py}[1]{{\tt #1}}
\newcommand{\att}[1]{{\tt #1}}


\newcommand{\av}[1]{{\color{ansa} AV: #1}}
\newcommand{\avout}[2][]{{\color{ansa} AV:}{\color{lightgray}\textbf{#2}} {\color{ansa} #1}}
\newcommand{\jv}[1]{{\color{svlinks} JV: #1}}
\newcommand{\dismiss}[1]{}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{PyVallex: A Processing System for valency Lexicon Data} % The article title
% \author{
% 	\authorstyle{Anna Vernerov\'a\textsuperscript{1} and Jonathan L. Verner\textsuperscript{2,3}} % Authors
% 	\newline\newline % Space before institutions
% 	\textsuperscript{1}\institution{Institute of Formal and Applied Linguistics, Charles University, Prague}\\ % Institution 1
% 	\textsuperscript{2}\institution{Department of Logic, Charles University, Prague}\\ % Institution 2
% 	\textsuperscript{3}\institution{Institute of Formal and Applied Linguistics, Charles University, Prague} % Institution 3
% }


%% \av{KTEREJ BLB TOHLE VYMYSLEL?}
% \name{Author1, Author2, Author3}

% \address{Affiliation1, Affiliation2, Affiliation3 \\
%      Address1, Address2, Address3 \\
%      author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
%      \{author1, author5, author9\}@abc.org\\}

\name{Anna Vernerov\'a\textsuperscript{1}, Jonathan L. Verner\textsuperscript{2,1}}

\address{\textbf{1:} Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics\\
         \textbf{2:} Charles University, Faculty of Arts, Department of Logic\\
         \textbf{1:} Malostransk{\'{e}} n{\'{a}}m{\v{e}}st{\'{i}} 25, 118 00 Prague 1, Czech Republic;
         \textbf{2:} Celetn\'a 20, 110 00 Prague 1, Czech Republic\\
         vernerova@ufal.mff.cuni.cz, jonathan@temno.eu\\}


% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

%\maketitle % Print the title
%\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\abstract{
\av{150-200 slov}
We present PyVallex, a Python-based system for presenting, searching/filtering, editing/extending and processing
of valency lexicon data originally available in a text-based format. The system consists of several components:
a parser for the specific lexicon format used in several valency lexicons,
a data-validation framework, a regular expression based search engine,
a map-reduce style framework for querying the lexicon data and a web-based interface integrating
complex search and some basic editing capabilities.
PyVallex provides most of the typical functionalities of a Dictionary Writing System (DWS),
such as multiple presentation modes for the underlying lexical database,
automatic evaluation of consistency tests,
and a mechanism of merging updates coming from multiple sources.
The editing functionality of the web-based interface is currently limited to edits of existing lexical entries,
but additional script-based operations on the database are also possible.
The code is published under the open source MIT license and is also available in the form of a Python module
for integrating into other software.
\newline \Keywords{\av{3-5 v poradi dle preference: dictionary management, lexicographic software, electronic dictionaries, dictionary writing system, valency lexicons}}
}
\maketitleabstract


%----------------------------------------------------------------------------------------
%	PREVIOUS WORK
%----------------------------------------------------------------------------------------
\section{When The Old Tools No Longer Suffice}
\av{
In the early years of this millenium,
the valency lexicon Vallex started being developed at the Institute of Formal and Applied Linguistics in Prague
(then Center for Computational Linguistics)
with the aims of
validating the existing valency theory of the Functional Generative Description and
pursuing further syntactic research
\cite{LopatkovaEtAl02tektogramaticky}.
Early on, a lexicon named PDT-Vallex was forked from Vallex and used for
maintaining consistency of the tectogrammatical annotation of the data in the
Prague Dependency Treebank 2.0 \cite{PDT2.0,HajicHonetschlager03annotation}.
While the PDT-Vallex was fully integrated with the tree editor and search tool TrEd%
\footnote{\url{https://ufal.mff.cuni.cz/tred/}}
\cite{PajasStepanek08recent}
and its native format is now XML-based,
the Vallex project never really moved away from the early technological solution based on a text-based format
processed by a sequence of scripts that led to an intermediate representation as XML-data
\cite{Zabokrtsky05valency}
and final presentations as web-pages (with individual lexicon entries stored in individual html files)
and a pdf/printed lexicon (produced via TeX).
The annotators are reluctant to move away from the text-based format which they can use with a common text-editor
(with custom syntax highlighting definition files relying on a set of markup conventions);
this situation is not unique to the Vallex project, e.g.\ \cite{Benko19LexiCorp}.
On the other hand, the set of custom scripts for format conversions,
validity checking, and custom search,
making use of nowadays less known languages
such as perl, awk, and xslt
and showing traces of personal quirks
of a number of developers who are no longer on the project,
has, over the years, become disorganized and close to unmanageable.
}
\jv{
In the early 2000s researchers at the Institute of Formal and Applied Linguistics in Prague
(then the Center for Computational Linguistics) started developing the valency lexicon Vallex aiming to
validate the existing valency theory of the Functional Generative Description \cite{Sgall:1986} and to pursue
further syntactic research \cite{LopatkovaEtAl02tektogramaticky}.
Early on, a lexicon named PDT-Vallex was forked from Vallex and used for
maintaining consistency of the tectogrammatical annotation of the data in the
Prague Dependency Treebank 2.0 \cite{PDT2.0,HajicHonetschlager03annotation}.
While the PDT-Vallex was fully integrated with the tree editor and search tool TrEd%
\footnote{\url{https://ufal.mff.cuni.cz/tred/}}
\cite{PajasStepanek08recent}
and its native format is now XML-based,
the Vallex project never really moved away from the early technological solution based on a text-based format
processed by a sequence of scripts that led to an intermediate representation as XML-data
\cite{Zabokrtsky05valency}
and final presentations as web-pages (with individual lexicon entries stored in individual html files)
and a pdf/printed lexicon (produced via TeX).
The annotators are reluctant to move away from the text-based format which they can use with a common text-editor
(with custom syntax highlighting definition files relying on a set of markup conventions);
this situation is not unique to the Vallex project, e.g.\ \cite{Benko19LexiCorp}.
On the other hand, the set of custom scripts for format conversions,
validity checking, and custom search are not well documented and have a significant
% maintenance cost. Additionally the scripts are written using tools/languages which are not as widely
% popular today as they were when the project started making it hard for newer people to take part
% in development.
% They rely on tools like awk and xslt which
% making use of nowadays less known languages
% such as perl, awk, and xslt
% and showing traces of personal quirks
% of a number of developers who are no longer on the project,
% has, over the years, become disorganized and close to unmanageable.
}

% \av{\textbf{Markéta by se asi zlobila, že to píšu, jako bychom v tom měli totální chaos.
% Zvládl bys to nějak zjemnit, ale zachovat pointu?}}


%----------------------------------------------------------------------------------------
%	DESIGN OF THE SYSTEM
%----------------------------------------------------------------------------------------
\section{Motivation \& Design}
As noted above, the main motivation for designing a system from scratch was that the mostly unstructured
nature of the previous system made it difficult to add new functionality and maintain the
system. The design of the system did not allow for easy integration into other tools which
was compounded by almost non-existing documentation. On the other hand, the older system
provided significant functionality which the new system needed to replicate. An additional
strict requirement was that annotators could continue working as they were used to,
in other words, using the text-based data format and Subversion revision system, for creating
new entries as well as editing old ones.

The above considerations lead to the following design goals for the new system:

\paragraph{Modularity} To make adding new functionality easier, the system should be structured
into components, each with a well defined public interface (API). The components should only communicate
with each other through this API. This allows any new functionality to modify only the
relevant parts of the system and thus lowers the barrier for new contributors---a developer does
not need to have precise knowledge of the whole system to add/modify functionality to/of a single component.

\paragraph{Extensibility}
The system should provide as much as possible of the functionality
already present in the older system, and moreover be designed from the start to be flexible and easily
extensible. In particular, it should have a well-defined data-model and provide an interface to allow
accessing the data from other programs (either through a library interface or, e.g., a REST API).

\paragraph{Maintainability} The design should aim to minimize the maintenance cost of the system.
In particular the system should be well documented, the components should be covered by an automated
test suite and the code style should be as uniform as possible and follow best practices.

\paragraph{} While many of the needed functionalities could be achieved through existing Dictionary Writing Systems
(WDS; TODO: nejake odkazy), these systems were ruled out by the requirement that annotators be allowed
to continue using their current workflows.

Given the above, we have decided to implement the system in the \emph{Python 3} programming
language. Its advantage over \emph{Perl} (the main language of the previous system) is that it provides
much better language support for structured programming. It is also becoming much more popular in
the NLP community, which makes it more likely that new contributors will be able to work with the
code. Although Python (as is the case also for Perl) is a weakly typed language, we have opted
to enforce the use of type hints via the \emph{mypy} static type checker \cite{tool:mypy} run before every commit.
This makes the code effectively strongly-typed. In the interest of maintainability, we also enforce
a uniform coding style through the use of a commit-hook which runs \emph{autopep} \cite{tool:autopep8}
on the commited code.

We expect that most annotators will learn to use selected features of PyVallex over time, especially the search/filtering functions, tabular output and editing of existing lexical units.
The old system of custom-made scripts will be used alongside PyVallex for some time, until all of its functionalities are fully implemented and tested.


%----------------------------------------------------------------------------------------
%	THE DATA LAYER
%----------------------------------------------------------------------------------------
\section{Data Layer}
A valency lexicon consists of a collection of \emph{Lexemes}. A Lexeme represents a group of related
lexical units that share the same lemma or (as in the case of Vallex, NomVallex, and other lexicons) a group of derivationally related lemmas. Each lexical unit corresponds to a single meaning of these lemmas. Each lexical unit can be annotated
with a number linguistic properties,
e.g.\
semantic (a gloss of the given meaning,
indication of primary and metaphorical meanings),
syntactic (does the given LU enter syntactic structures such as passive, reflexive and reciprocal constructions?),
and features specific to valency lexicons
(the valency frame and annotation of individual valency complementations with a functor, obligatoriness and a list of possible forms of expressions;
indication of control, i.e.\ for a given lexical unit realizing one of its complementations through an infinitive,
indicate which other valency complementation is referentially identical with the subject of this infinitive).

The data layer definition is provided in the \py{vallex.data\_structures} module and consists of the
following classes:
\py{Lexicon} (representing a collection of \emph{lexemes}), \py{Lexeme} (representing
a single \emph{lexeme}), \py{LexicalUnit} (representing a \emph{lexical unit}, i.e.\ a single meaning), \py{Attrib} (representing a linguistically relevant property of a \emph{lexical unit}). The \py{Attrib} class also has several specializations:
\py{Frame} (representing the \emph{valency frame}, represented as a list of \emph{complementations}),
\py{Lemma} (representing the \emph{lemma} or set of lemmas), and
\py{SpecVal} (representing the changes between the valency of two related units,
e.g.\ a verb and a deverbal noun or a verb and its translation to another language).
It is expected that further specializations
will be defined to deal with particular properties.

Each of the above classes can store textual comments (which can be used to explain
the reasoning behind a specific annotation, mark the data element as work in progress, etc). They can also
store the original unparsed form (in the original text-based format) together with its location in the
source files and they all provide a method to convert the data into a JSON representable structure.

%----------------------------------------------------------------------------------------
%	CORE COMPONENTS
%----------------------------------------------------------------------------------------
\section{Core components}
The core of the system consists of a \emph{parser module} which takes care of parsing the data and
constructing an in-memory representation of it---the data layer; a \emph{search module} which
provides query capabilities; a \emph{script module} which provides a framework for running data validation
and batch processing scripts; and an \emph{output module} which converts the in-memory representation into
various output formats. Each of these components is implemented in a python submodule of the main \py{vallex}
module and the components are mostly independent of each other.

%------------------------------------------------
\subsection{The parser}
The parse module takes care of parsing the specialized format (\autoref{fig:txt-fmt}) used by annotators when creating the lexicon data. The format %(described in \cite{vallex})  %%AV: existuji jen popisy jeho semantiky, ale nikde neni popis primo tohoto formatu, protoze se povazuje za ciste interni
is designed to be easily editable in any text editor and concise enough to facilitate manual creation.

\begin{figure}
\small
\begin{verbatim}
: id: blu-v-brát-vzít-1
  ~ impf: brát (si) pf: vzít (si)
  + ACT(1;obl) PAT(4;obl) ORIG(od+2;opt)
    LOC(;typ) DIR1(;typ) RCMP(za+4;typ)
  -synon: impf: přijímat; získávat
          pf: přijmout; získat
  -example: impf: brát si od někoho mzdu
     pf: vzal si od něj peníze za práci
  -note: volné si
     mohli brát na odměnách.COMPL
           měsíčně 26 až 40 tisíc korun
  -recipr: ACT-ORIG
  -diat: no_poss_result no_recipient
     pf: deagent: peníze navíc se
                musí odněkud vzít
         passive
     impf: deagent
       passive za práci se bere mzda
\end{verbatim}
\caption{\label{fig:txt-fmt}Sample lexical unit in the textual format (simplified)}
\end{figure}

The parser itself is split into a tokenizer function producing a stream of tokens and several
parse methods. The methods take care of constructing the \py{Lexeme}, \py{LexicalUnit}
and \py{Attrib} classes. It is designed in such a way that adding a specialized parser for
a newly introduced property is just a matter of writing a single function which is given
the body of the attribute and returns an instance of (a descendant of) the \py{Attrib} class.
To integrate the function into the parser, it is enough to decorate it with a provided
Python decorator.

%------------------------------------------------
\subsection{Search}
The search module provides query capabilities to the system.

\paragraph{Queries} A query is a collection of conditions
and the result of the query is a set of lexical units each of which meets all
of the conditions of the query. Each condition consists of a \emph{selector} and a
\emph{pattern}. For each lexical unit the \emph{selector} is passed to a \py{match\_key\_values}
function which constructs a list of strings. A lexical unit satisfies the condition if at
least one of the strings matches the \emph{pattern}, which is a regular expression.
The \emph{selector} is a dot-separated list of strings. The standard implementation of the
\py{match\_key\_values} function interprets the first element of the selector as an attribute
name which it retrieves. It then passes the rest of the selector to the attribute's \py{match\_key\_values}
function to construct the list of strings to be matched against. The standard implementation
of the attribute's \py{match\_key\_values} method returns the attribute's textual representation
if the \emph{selector} is empty, its source form if the selector is 'src' and otherwise treats
the selector as a path through the attributes structure treated as a tree. It resolves the path and returns the
value present in the relevant node. For example a query consisting of the single condition
\begin{verbatim}
    example.pf=.*od.*
\end{verbatim}
would match the lexical unit shown in \autoref{fig:txt-fmt} whose \att{example} attribute
has the following structure
\begin{verbatim}
  {
    'impf': [ 'brát si od někoho mzdu'],
    'pf': ['vzal si od něj peníze za
            práci']
  }
\end{verbatim}
The \emph{selector} \att{example.pf} would retrieve the \att{example} attribute and from it
its \att{pf} node which contains the string {\tt vzal si od něj peníze za práci}. This string
incidentally matches the regular expression {\tt .*od.*}.

For discoverability purposes, each attribute has a method which returns a lists of all valid paths
inside its structure.

\paragraph{Executing queries}
The search module contains a \py{grep} method to execute queries. It additionally contains a \py{filter}
method which allows pruning each lexical unit from the result set so that it contains only the properties/attributes a user is interested in. Another method is provided to compute various histograms.
The histogram method takes three arguments: the first argument is a collection of lexical units over which the histogram is computed. The second argument is a \emph{selector} which produces a list of strings
from each lexical unit in the same way as is done when evaluating queries. The last argument is a
regular expression which extracts the values to be counted from each string from the list. \autoref{fig:histogram}
shows how the UI displays the results of computing the histogram for the {\tt frame.functor} selector with
the trivial pattern ({\tt .*}).
\begin{figure}
    \includegraphics[width=\hsize]{images/histogram.png}
    \caption{\label{fig:histogram}The web-based UI showing a histogram of the frame functors}
\end{figure}

%------------------------------------------------
\subsection{Scripts}
The script module (\py{vallex.scripts}) provides a framework for running simple procedures over the
valency lexicon data. The scripts are loaded by the framework from a configurable directory. Each
file in this directory is a Python source file containing the definitions of the procedures.
The system currently recognizes five kinds of procedures: \emph{test}, \emph{transform}, \emph{compute} and \emph{map/reduce}.

\paragraph{Test} The test procedures are used to implement data validation for the lexicon. Each test
function receives a lexical unit as its argument.\footnote{Actually, there are four sub-types differing in what
argument is passed --- a collection of lexicons, a single lexicon, a single lexeme or a lexical unit;
for simplicity here and also in the paragraphs dedicated to other types of procedures we discuss only the sub-type receiving a lexical unit.}
It checks whether the unit satisfies the test and raises an appropriate exception if it doesn't. The framework
iterates over all lexical units passing them in turn to each test procedure and collecting the results. The
results are saved in the in-memory representation and can then be displayed to the user (\autoref{fig:test-result})
\begin{figure}
    \includegraphics[width=0.8\hsize]{images/test-result.png}
    \caption{\label{fig:test-result}The web-based UI showing the result of a data-validation run on a lexical unit.}
\end{figure}
The results are also annotated with the docstring of the test-procedure which can be used to provide human
readable explanation of the failed result. An example data validation test is provided in \autoref{fig:data-test-code}
\begin{figure}
\small
\begin{minted}{python}
def test_lu_acc_bez_deagent(lu):
  """
    Verbs that allow accusative form
    of one of their complementations
    must also allow the deagentive
    diathesis.
  """
  forms = sum(
     [fe.forms for fe in lu.frame.elements],
     []
  )
  has_acc = '4' in forms or
              'adj-4' in forms

  if not has_acc or lu.reflexive:
    raise TestDoesNotApply

  deagent = 'diat' in lu.attribs and
      'deagent' in lu.attribs['diat']._data

  if not deagent:
      raise TestFailed()
\end{minted}
\caption{\label{fig:data-test-code}An example of a data-validation procedure}
\end{figure}

\paragraph{Transform} The transform procedures can be used to implement one-time lexicon-wide changes, e.g.
renaming an attribute. They receive a lexical unit which they can modify and return.

\paragraph{Compute} The compute procedures are similar to the transform procedures, but are used to implement dynamically computed properties which are \emph{not} saved back to the lexicon on disk.

\paragraph{Map/Reduce} The map/reduce procedures are used to perform more complicated analyses of the lexicon
for which a simple search/histogram does not suffice. Each map/reduce procedure consists of a pair of functions:
a \emph{mapper} and an (optional) \emph{reducer}. Each mapper receives a lexical unit as an argument and
uses a framework-provided \py{emit} function to emit a collection of \py{(key, value)} pairs. The framework
iterates over all lexical-units passing them to the mapper functions and collecting the resulting pairs. It then groups them by the \py{key} component and passes the groups to the reducer which can do further processing\avout{ on them}\av{//which can be used for further processing}.
A default reducer which just counts the number of values for a given key is provided by the framework and is
used when no specialized reducer is provided by the user. Note that, although map/reduce is now connected with
parallel processing, here all the mappers and reducers are run sequentially since the small size of the data \avout[justifies sequential processing]{makes
sequential processing completely adequate}. \avout[W]{Instead, w}e use map/reduce as a familiar methodology for structuring
analysis code.

%------------------------------------------------
\subsection{Output}

The output module provides tools to export lexicon data in various formats. In addition to built-in JSON
output (which is implemented in the data-layer), new formats can be defined using \emph{Jinja2} \cite{tool:Jinja2} templates.
Currently only a single txt format is provided which outputs the in-memory representation in the same
format that the annotators use. This can be used to check the fidelity of the in-memory representation
(by comparing it with the original source) and for normalizing the sources. In the future other formats
may be added, e.g., XML.

%----------------------------------------------------------------------------------------
%	THE UI LAYER
%----------------------------------------------------------------------------------------

\section{The UI Layer}

Although the PyVallex system provides a command line UI for performing searches, computing histograms
and running batch scripts, it is expected that most of its users will prefer a nicer graphical user interface.
We have decided to provide the GUI as a web-based interface. This has several advantages. First, it allows
the system to be installed on a server and be accessible to users without forcing them to aquire the comparatively large datasets or requiring them to maintain the installation. Moreover, web-based technologies are very
common and basing the UI on them considerably lowers the barrier for new contributors/maintainers. It
is expected that even a person without a detailed knowledge of the system would be able to contribute simple modifications to the UI in a short amount of time. Finally, using a webview widget provided by the
\emph{Qt library} \cite{tool:Qt},
we can implement a simple local client based on the same code-base.

\subsection{Implementation}
The server-backend is a simple Python \emph{WSGI} \cite{tool:WSGI} application written in the \emph{Bottle.py} \cite{tool:bottle.py}
microframework. It uses an \emph{SQLite} database \cite{tool:SQLite} to store a JSON representation of the in-memory
data\footnote{The database is used solely as a method to allow safe concurrent access to the data.} and exposes a REST-based api which is consumed by the front end. The front end is a Javascript application written using the
\emph{Vue} framework \cite{tool:Vue.js}  together with the 
\emph{Vuetify} component library \cite{tool:Vuetify} to provide a familiar
\emph{Polymer}-style interface \cite{tool:polymer}.

\subsection{User Features}
\subsubsection{Searching/Filtering}
By default, all lexemes in the lexicon are displayed in the alphabetical order.
However, it is possible to search for lexical units based on their attributes 
(the \texttt{Attrib} class provided by the parser, \secref{parser}),
\begin{figure}
    \includegraphics[width=\hsize]{images/ui-search.png}
    \caption{\label{fig:ui-search}The search field}
\end{figure}

\subsubsection{View modes}
Currently, two view modes are provided:
in the Source View mode, the complete textual representation of the give

\begin{figure}
    \includegraphics[width=\hsize]{images/ui-lexical-unit.png}
    \caption{\label{fig:ui-lexical-unit}The display of a lexical unit}
\end{figure}

\begin{figure}
    \includegraphics[width=\hsize]{images/ui-lexical-unit-src.png}
    \caption{\label{fig:ui-lexical-unit-src}The display of a lexical unit (source view)}
\end{figure}



\begin{figure}
    \includegraphics[width=\hsize]{images/ui-editing.png}
    \caption{\label{fig:ui-editing}A simple editor}
\end{figure}



%----------------------------------------------------------------------------------------
%	FUTURE WORK
%----------------------------------------------------------------------------------------


\section{Future work}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

\section{Bibliographical References}
\label{main:ref}

\bibliographystyle{lrec}
\bibliography{example}

%----------------------------------------------------------------------------------------

\end{document}
