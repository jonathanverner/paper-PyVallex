%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

\usepackage{textalpha} % For unicode greek letters in normal text
\usepackage{amssymb, amsmath, amsthm, mathrsfs, enumitem, amsfonts, latexsym, bbm, stmaryrd, thmtools}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\definecolor{svlinks}{rgb}{.0,0.3,0.6} %tmavě modrá
\usepackage[bookmarks,colorlinks=true,pdfhighlight=/O,linkcolor=svlinks,urlcolor=svlinks,citecolor=svlinks,
            pdftitle={PyVallex: A Processing System for Vallency Lexicon Data},
            pdfauthor={Anna Vernerová, Jonathan L. Verner},
            pdfsubject={},
            pdfkeywords={}
            ]{hyperref}
\usepackage{enumitem}
\usepackage{minted}

\newcommand{\py}[1]{{\tt #1}}
\newcommand{\att}[1]{{\tt #1}}


\newcommand{\AV}[1]{{\color{red} AV: #1}}
\newcommand{\JV}[1]{{\color{svlinks} JV: #1}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{PyVallex: A Processing System for Vallency Lexicon Data} % The article title

\author{
	\authorstyle{Anna Vernerov\'a\textsuperscript{1} and Jonathan L. Verner\textsuperscript{2,3}} % Authors
	\newline\newline % Space before institutions
	\textsuperscript{1}\institution{Institute of Formal and Applied Linguistics, Charles University, Prague}\\ % Institution 1
	\textsuperscript{2}\institution{Department of Logic, Charles University, Prague}\\ % Institution 2
	\textsuperscript{3}\institution{Institute of Formal and Applied Linguistics, Charles University, Prague} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{We present PyVallex,
a Python-based system for display, editing and processing
of vallency lexicon data. The system consists of several components:
a parser for the specialized lexicon format used in several vallency lexicons,
a data-validation framework, a regular expression based search engine,
a map-reduce style framework for querying the lexicon data and a web-based interface integrating
complex search and some basic editing capabilities. Several of the components are provided
as a Python package allowing for programmatic access to the data.
}

%----------------------------------------------------------------------------------------
%	PREVIOUS WORK
%----------------------------------------------------------------------------------------
\section{Previous work}

%----------------------------------------------------------------------------------------
%	DESIGN OF THE SYSTEM
%----------------------------------------------------------------------------------------
\section{Motivation \& Design}
The main motivation for designing a system from scratch was that the mostly unstructured
nature of the previous system made it difficult to add new functionality and maintain the
system. The design of the system did not allow for easy integration into other tools which
was compounded by almost non-existing documentation. On the other hand, the older system
provided significant functionality which the new system needed to replicate. An additional
strict requirement was that annotators could continue working as they were used to,
in other words, using the plaintext data format and Subversion revision system, for creating
new entries as well as editing new ones.

The above considerations lead to the following design goals for the new system:

\paragraph{Modularity} To make adding new functionality easier, the system should be structured
into components, each with a well defined public interface (API). The components should only communicate
with each other through this API. This allows any new functionality to modify only the
relevant parts of the system and thus lowers the barrier for new contributors---a developer does
not need to have precise knowledge of the whole system to add/modify functionality to a single component.

\paragraph{Extendability} While the system should provide as much as possible of the functionality
already present in the older system, it should be designed from the start to be flexible and easily
extendible. In particular, it should have a well-defined data-model and provide an interface to allow
accessing the data from other programs (either through a library interface or, e.g., a REST API).

\paragraph{Maintainability} The design should aim to minimize the maintenance cost of the system.
In particular the system should be well documented, the components should be covered by an automated
test suite and the code style should be as uniform as possible and follow best practices.

\paragraph{} While many of the needed functionalities could be achieved through existing Dictionary Writing Systems
(WDS; TODO: nejake odkazy), these systems were ruled out by the requirement that annotators be allowed
to continue using their current workflows.

Given the above, we have decided to implement the system in the \emph{Python 3} programming
language. Its advantage over \emph{Perl} (the language of the previous system) is that it provides
much better language support for structured programming. It is also becoming much more popular in
the NLP community, which makes it more likely that new contributors will be able to work with the
code. Although Python (as is the case also for Perl) is a weakly typed language, we have opted
to enforce the use of type hints via the mypy static type checker (\cite{tool:mypy}) run before every commit.
This makes the code effectively strongly-typed. In the interest of maintainability, we also enforce
a uniform coding style through the use of a commit-hook which runs autopep (\cite{tool:autopep8})
on the commited code.

We expect that most annotators will learn to use selected features of PyVallex over time, especially the search/filtering functions, tabular output and editing of existing lexical units.
The old system of custom-made scripts will be used used alongside PyVallex for some time, until all of its functionalities are fully implemented and tested.


%----------------------------------------------------------------------------------------
%	THE DATA LAYER
%----------------------------------------------------------------------------------------
\section{Data Layer}
A vallency lexicon consists of a collection of \emph{Lexemes}. A Lexeme represents a group of related
lexical units that share the same lemma or (as in the case of Vallex, NomVallex, and other lexicons) a group of derivationally related lemmas. Each lexical unit corresponds to a single meaning of these lemmas. Each lexical unit can be annotated
with a number linguistic properties,
e.g. \AV{zpetne lomitko pred.mezeru}
semantic (a gloss of the given meaning, level of abstraction,
indication of primary and metaphorical meanings),
syntactic (does the given LU enter syntactic structures such as passive, reflexive and reciprocal constructions? Which valency element of the given verb is referentially identical with the subject of the dependent infintive (a phenomenon called control)?
\AV{mozna moc podrobne a zase zahodit}
\JV{myslim, ze je to fajn, at je tam taky trochu lingvistiky}

The data layer definition is provided in the \py{vallex.data\_structures} module and consists of the
following classes \py{Lexicon} (representing a collection of \emph{lexemes}), \py{Lexeme} (representing
a single \emph{lexeme}), \py{LexicalUnit} (representing a \emph{lexical unit}), \py{Attrib} (representing a linguistically relevant property of a \emph{lexical unit}). The \py{Attrib} class also has several specializations: \py{Frame}
(representing the \emph{frame} annotation), \py{Lemma} (representing the \emph{lemma} annotation) and \py{SpecVal}
(representing the changes between a verb-noun derived pair). It is expected, that further sepcializations
will be defined to deal with particular properties.

Each of the above classes can store annotator provided textual comments (which can be used to explain
the reasoning behind a specific annotation, mark the data element as a work in progress, etc). They can also
store the original unparsed form (in the original plaintext format) together with its location in the
source and they all provide a method to convert the data into a JSON representable structure.

%----------------------------------------------------------------------------------------
%	CORE COMPONENTS
%----------------------------------------------------------------------------------------
\section{Core components}
The core of the system consists of a \emph{parser module} which takes care of parsing the data and
constructing an in-memory representation of it---the data layer; a \emph{search module} which
provides query capabilities; a \emph{script module} which provides a framework for running data validation
and batch processing scripts; and an \emph{output module} which converts the in-memory representation into
various output formats. Each of these components is implemented in a python submodule of the main \py{vallex}
module and the components are mostly independent of each other.

%------------------------------------------------
\subsection{The parser}
The parse module takes care of parsing the specialized format (\autoref{fig:txt-fmt}) used by annotators when creating the lexicon data. The format (described in \cite{vallex}) is designed to be easily editable in any text editor and consise enough to facilitate manual creation.

\begin{figure}
\tiny
\begin{verbatim}
    : id: blu-v-brát-vzít-1
            ~ impf: brát (si) pf: vzít (si) iter: brávat (si)
            + ACT(1;obl) PAT(4;obl) ORIG(od+2;opt) LOC(;typ) DIR1(;typ) RCMP(za+4;typ)
                -synon: impf: přijímat; získávat pf: přijmout; získat
                -example: impf: brát si od někoho mzdu / peníze za práci;
                        pf: vzal si od něj peníze za práci;
                -note: mohli loni brát na odměnách.COMPL měsíčně 26 až 40 tisíc korun
                    volné si
                -recipr: impf: ACT-ORIG %berou si od sebe peníze%
                        pf: ACT-ORIG %nikdy si od sebe nevzali peníze%
\end{verbatim}
\caption{\label{fig:txt-fmt}Sample lexical unit in the textual format}
\end{figure}

The parser itself is split into a tokenizer function producing a stream of tokens and several
parse methods. The methods take care of constructing the \py{Lexeme}, \py{LexicalUnit}
and \py{Attrib} classes. It is designed in such a way that adding a specialized parser for
a newly introduced property is just a matter of writing a single function which is given
the body of the attribute and returns an instance of (a descendant) the \py{Attrib} class.
To integrate the function into the parser, it is enough to decorate it with a provided
Python decorator.

%------------------------------------------------
\subsection{Search}
The search module provides query capabilities to the system.

\paragraph{Queries} A query is a collection of conditions
and the result of the query is a set of lexical units each of which meets all
of the conditions of the query. Each condition consists of a \emph{selector} and a
\emph{pattern}. For each lexical unit the \emph{selector} is passed to a \py{match\_key\_values}
function which constructs a list of strings. A lexical unit satisfies the condition if at
least one of the strings matches the \emph{pattern}, which is a regular expression.
The \emph{selector} is a dot-separated list of strings. The standard implementation of the
\py{match\_key\_values} function interprets the first element of the selector as an attribute
name which it retrieves. It then passes the rest of the selector to the attribute's \py{match\_key\_values}
function to construct the list of strings to be matched against. The standard implementation
of the attribute's \py{match\_key\_values} method returns the attribute's textual representation
if the \emph{selector} is empty, its source form if the selector is 'src' and otherwise treats
the selector as a path through the attributes structure treated as a tre. It resolves the path and returns the
value present in the relevant node. For example a query consisting of the single condition
\begin{verbatim}
    example.pf=.*od.*
\end{verbatim}
would match the lexical unit shown in \autoref{fig:txt-fmt} whose \att{example} attribute
has the following structure
\begin{verbatim}
    {
        'impf': [ 'brát si od někoho mzdu /
                   peníze za práci'],
        'pf': ['vzal si od něj peníze za
                práci']
    }
\end{verbatim}
The \emph{selector} \att{example.pf} would retrieve the \att{example} attribute and from it
its \att{pf} node which contains the string {\tt vzal si od něj peníze za práci}. This string
incidentally matches the regular expression {\tt .*od.*}.

For discoverability purposes, each attribute has a method which returns a lists of all valid paths
inside its structure.

\paragraph{Executing queries}
The search module contains a \py{grep} method to execute queries. It additionally contains a \py{filter}
method which allows pruning each lexical unit from the result set so that it contains only the properties/attributes a user is interested in. Another method is provided to compute various histograms.
The histogram method takes three arguments: the first argument is a collection of lexical units over which the histogram is computed. The second argument is a \emph{selector} which produces a list of strings
from each lexical unit in the same way as is done when evaluating queries. The last argument is a
regular expression which extracts the values to be counted from each string from the list. \autoref{fig:histogram}
shows how the UI displays the results of computing the histogram for the {\tt frame.functor} selector with
the trivial pattern ({\tt .*}).
\begin{figure}
    \includegraphics[width=\hsize]{images/histogram.png}
    \caption{\label{fig:histogram}The web ui showing a histogram of the frame functors}
\end{figure}

%------------------------------------------------
\subsection{Scripts}
The script module (\py{vallex.scripts}) provides a framework for running simple procedures over the
vallency lexicon data. The scripts are loaded by the framework from a configurable directory. Each
file in this directory is a Python source file containing the definitions of the procedures. Currently
the system recognizes five kinds of procedures: \emph{test}, \emph{transform}, \emph{compute} and \emph{map/reduce}.

\paragraph{Test} The test procedures are used to implement data validation for the lexicon. Each test
function receives a lexical unit as its argument\footnote{Actually, there are four sub-types differing in what
argument is passed --- a collection of lexicons, a single lexicon, a single lexeme or a lexical unit; for simplicity here and also in the other types we consider only the sub-type receiving a lexical unit.}. It is expected
to check whether the unit satisfies the test and raise an appropriate exception if it doesn't. The framework
iterates over all lexical units passing them in turn to each test procedure and collecting the results. The
results are saved in the in-memory representation and can then be displayed to the user (\autoref{fig:test-result})
\begin{figure}
    \includegraphics[width=0.8\hsize]{images/test-result.png}
    \caption{\label{fig:test-result}The web ui showing the result of a data-validation run on a lexical unit.}
\end{figure}
The results are also annotated with the docstring of the test-procedure which can be used to provide human
readable explanation of the failed result. An example data validation test is provided in \autoref{fig:data-test-code}
\begin{figure}
\begin{minted}{python}
def test_lu_acc_bez_deagent(lu):
    """
        Slovesa, která mají v rámci akuzativ,
        musí mít uvedeno deagent.
    """
    forms = sum([fe.forms
                 for fe in lu.frame.elements],
                [])
    has_acc = '4' in forms or 'adj-4' in forms

    if not has_acc or lu.reflexive:
        raise TestDoesNotApply

    deagent = 'diat' in lu.attribs and
              'deagent' in lu.attribs['diat']._data

    if not deagent:
        raise TestFailed()
\end{minted}
\caption{\label{fig:data-test-code}An example of a data-validation procedure}
\end{figure}

\paragraph{Transform} The transform procedures can be used to implement one-time lexicon-wide changes, e.g.
renaming an attribute. They receive a lexical unit which they can modify and return.

\paragraph{Compute} The compute procedures are similar to the transform procedures, but are used to implement dynamically computed properties which are \emph{not} saved back to the lexicon on disk.

\paragraph{Map/Reduce} The map/reduce procedures are used to perform more complicated analyses of the lexicon
for which a simple search/histogram does not suffice. Each map/reduce procedure consists of a pair of functions:
a \emph{mapper} and an (optional) \emph{reducer}. Each mapper receives a lexical unit as an argument and
uses a framework-provided \py{emit} function to emit a collection of \py{(key, value)} pairs. The framework
iterates over all lexical-units passing them to the mapper functions and collecting the resulting pairs. It then groups them by the \py{key} component and passes the groups to the reducer which can do further processing on them.
A default reducer which just counts the number of values for a given key is provided by the framework and is
used when no specialized reducer is provided by the user. Note that, although map/reduce is now connected with
parallel processing, here all the mappers and reducers are run sequentially since the small size of the data makes
sequential processing completely adequate. Instead, we use map/reduce as a familiar methodology for structuring
analysis code.

%------------------------------------------------
\subsection{Output}

The output module provides tools to export lexicon data in various formats. In addition to built-in JSON
output (which is implemented in the data-layer), new formats can be defined using \cite{tool:Jinja2} templates.
Currently only a single txt format is provided which outputs the in-memory representation in the same
format that the annotators use. This can be used to check the fidelity of the in-memory representation
(by comparing it with the original source) and for normalizing the sources. In the future other formats
may be added, e.g., XML.

%----------------------------------------------------------------------------------------
%	THE UI LAYER
%----------------------------------------------------------------------------------------

\section{The UI Layer}

Although the PyVallex system provides a command line UI for performing searches, computing histograms
and running batch scripts it is expected that most of its users will prefer a nicer graphical user interface.
We have decided to provide the GUI as a web-based interface. This has several advantages. First, it allows
the system to be installed on a server and be accessible to users without forcing them to aquire the comparatively large datasets or requiring them to maintain the installation. Moreover, web-based technologies are very
common and basing the UI on them considerably lowers the barrier for new contributors/maintainers. It
is expected that even a person without a detailed knowledge of the system would be able to contribute simple modifications to the UI in a short amount of time. Finally, using a webview widget provided by the \cite{tool:Qt}
library we can implement a simple local client based on the same code-base.

\subsection{Implementation}
The server-backend is a simple Python \cite{tool:WSGI} application written in the \cite{tool:bottle.py}
microframework. It uses a SQLite database \cite{tool:SQLite} to store a JSON representation of the in-memory
data\footnote{The database is used solely as a method to allow safe concurrent access to the data.} and exposes a REST-based api which is consumed by the front end. The front end is a Javascript application written using the
\cite{tool:Vue.js} Vue framework together with the \cite{tool:Vuetify} component library to provide a familiar
\cite{tool:polymer} Polymer-style interface.

\subsection{User Features}


\begin{figure}
    \includegraphics[width=\hsize]{images/ui-search.png}
    \caption{\label{fig:ui-search}The search field}
\end{figure}

\begin{figure}
    \includegraphics[width=\hsize]{images/ui-lexical-unit.png}
    \caption{\label{fig:ui-lexical-unit}The lexical unit display}
\end{figure}

\begin{figure}
    \includegraphics[width=\hsize]{images/ui-lexical-unit-src.png}
    \caption{\label{fig:ui-lexical-unit-src}The lexical unit display (source view)}
\end{figure}

\begin{figure}
    \includegraphics[width=\hsize]{images/ui-editing.png}
    \caption{\label{fig:ui-editing}A simple editor}
\end{figure}



%----------------------------------------------------------------------------------------
%	FUTURE WORK
%----------------------------------------------------------------------------------------


\section{Future work}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
